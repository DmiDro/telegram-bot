import os
import logging
import asyncio
from openai import OpenAI, AsyncOpenAI
from openai._httpx import AsyncHttpxClient

# üîê Railway ‚Üí Set in Variables
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
OPENAI_PROXY = os.environ.get("OPENAI_PROXY")

# –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
sync_client = OpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://api.openai.com/v1",
    proxies={"http": OPENAI_PROXY, "https": OPENAI_PROXY} if OPENAI_PROXY else None
)

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
async_client = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    base_url="https://api.openai.com/v1",
    http_client=AsyncHttpxClient(proxy=OPENAI_PROXY) if OPENAI_PROXY else None
)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å fallback
async def generate_response(prompt: str, model: str = "gpt-4o") -> str:
    prompt_id = prompt.strip()[:60]
    try:
        logging.info(f"üì§ AsyncOpenAI: –æ—Ç–ø—Ä–∞–≤–∫–∞ prompt ({prompt_id})")
        response = await async_client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt.strip()}]
        )
        return response.choices[0].message.content.strip()

    except Exception as e_async:
        logging.error(f"‚ö†Ô∏è AsyncOpenAI –æ—à–∏–±–∫–∞: {e_async} ‚Äî fallback –Ω–∞ sync.")

        try:
            loop = asyncio.get_running_loop()
            response = await loop.run_in_executor(
                None,
                lambda: sync_client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt.strip()}]
                )
            )
            return response.choices[0].message.content.strip()

        except Exception as e_sync:
            logging.critical(f"‚ùå Sync fallback —Ç–æ–∂–µ —É–ø–∞–ª: {e_sync}")
            return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç OpenAI."
